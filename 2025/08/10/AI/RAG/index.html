<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32X32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16X16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7CZCOOL+XiaoWei:300,300italic,400,400italic,700,700italic%7CNoto+Serif+SC:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/orange/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"yoursite.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="引言：RAG——向量数据库+大语言模型">
<meta property="og:type" content="article">
<meta property="og:title" content="RAG">
<meta property="og:url" content="http://yoursite.com/2025/08/10/AI/RAG/index.html">
<meta property="og:site_name" content="Hynis">
<meta property="og:description" content="引言：RAG——向量数据库+大语言模型">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-09T17:54:36.000Z">
<meta property="article:modified_time" content="2025-08-10T14:25:12.975Z">
<meta property="article:author" content="Hynis">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://yoursite.com/2025/08/10/AI/RAG/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://yoursite.com/2025/08/10/AI/RAG/","path":"2025/08/10/AI/RAG/","title":"RAG"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>RAG | Hynis</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hynis</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">157</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">92</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">214</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">

<!-- 网易云外链-->
    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1439739102&auto=1&height=66"></iframe>
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>



      <div class="sidebar-panel-container">

        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#RAG"><span class="nav-number">1.</span> <span class="nav-text">RAG</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RAG%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">2.</span> <span class="nav-text">RAG的核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%A3%80%E7%B4%A2%E5%99%A8%EF%BC%88Retriever%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">1. 检索器（Retriever）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%94%9F%E6%88%90%E5%99%A8%EF%BC%88Generator%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">2. 生成器（Generator）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RAG%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">RAG的工作流程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RAG%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%85%A8%E8%B2%8C"><span class="nav-number">3.1.</span> <span class="nav-text">RAG系统架构全貌</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="nav-number">3.1.1.</span> <span class="nav-text">完整架构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%8F%AA%E6%98%AF%E2%80%9D%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-%E6%A8%A1%E5%9E%8B%E2%80%9D%EF%BC%9F"><span class="nav-number">3.1.2.</span> <span class="nav-text">为什么不只是”向量数据库+模型”？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%84%E7%BB%84%E4%BB%B6%E7%9A%84%E4%BD%9C%E7%94%A8%E5%8D%A0%E6%AF%94"><span class="nav-number">3.1.3.</span> <span class="nav-text">各组件的作用占比</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6"><span class="nav-number">4.</span> <span class="nav-text">开源框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">5.</span> <span class="nav-text">向量数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9F"><span class="nav-number">5.1.</span> <span class="nav-text">什么是向量数据库？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%90%91%E9%87%8F%E3%80%81%E5%B5%8C%E5%85%A5%E3%80%81%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">5.2.</span> <span class="nav-text">核心概念：向量、嵌入、相似度计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">5.3.</span> <span class="nav-text">向量数据库工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9F"><span class="nav-number">5.3.1.</span> <span class="nav-text">为什么需要向量数据库？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91"><span class="nav-number">5.3.2.</span> <span class="nav-text">性能考虑</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%82%E5%9C%BA%E4%B8%BB%E6%B5%81%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E6%AF%94"><span class="nav-number">5.4.</span> <span class="nav-text">市场主流向量数据库对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%BC%94%E7%A4%BA%E4%BB%A3%E7%A0%81"><span class="nav-number">5.5.</span> <span class="nav-text">向量数据库演示代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">6.</span> <span class="nav-text">语言大模型选择</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LangChain%E6%A1%86%E6%9E%B6%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="nav-number">7.</span> <span class="nav-text">LangChain框架深度解析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFLangChain%EF%BC%9F"><span class="nav-number">7.1.</span> <span class="nav-text">什么是LangChain？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LangChain%E6%9E%B6%E6%9E%84%E4%BD%93%E7%B3%BB"><span class="nav-number">7.2.</span> <span class="nav-text">LangChain架构体系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LangChain%E5%85%B3%E9%94%AE%E7%BB%84%E4%BB%B6%E8%AF%A6%E8%A7%A3"><span class="nav-number">7.3.</span> <span class="nav-text">LangChain关键组件详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%F0%9F%93%84-Document-Loaders%EF%BC%88%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8%EF%BC%89"><span class="nav-number">7.3.1.</span> <span class="nav-text">1. 📄 Document Loaders（文档加载器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E2%9C%82%EF%B8%8F-Text-Splitters%EF%BC%88%E6%96%87%E6%9C%AC%E5%88%86%E5%89%B2%E5%99%A8%EF%BC%89"><span class="nav-number">7.3.2.</span> <span class="nav-text">2. ✂️ Text Splitters（文本分割器）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%F0%9F%97%84%EF%B8%8F-Vector-Stores%EF%BC%88%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8%EF%BC%89"><span class="nav-number">7.3.3.</span> <span class="nav-text">3. 🗄️ Vector Stores（向量存储）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%F0%9F%94%97-Chains%EF%BC%88%E9%93%BE%EF%BC%89"><span class="nav-number">7.3.4.</span> <span class="nav-text">4. 🔗 Chains（链）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%F0%9F%A4%96-Agents%EF%BC%88%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%89"><span class="nav-number">7.3.5.</span> <span class="nav-text">5. 🤖 Agents（智能体）</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hynis"
      src="http://img.yesmylord.cn//1644852537960.jpg">
  <p class="site-author-name" itemprop="name">Hynis</p>
  <div class="site-description" itemprop="description">A blog about IT knowledge</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">214</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">157</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/YesYourHighness" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YesYourHighness" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1046467756@qq.com" title="E-Mail → mailto:1046467756@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://zouper.cn/" title="https:&#x2F;&#x2F;zouper.cn" rel="noopener" target="_blank">一杯好茶</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.klenkiven.xyz/" title="https:&#x2F;&#x2F;www.klenkiven.xyz&#x2F;" rel="noopener" target="_blank">KlenKiven</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hourunmeng.github.io/" title="https:&#x2F;&#x2F;hourunmeng.github.io&#x2F;" rel="noopener" target="_blank">润萌</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://flashxin.github.io/" title="https:&#x2F;&#x2F;flashxin.github.io&#x2F;" rel="noopener" target="_blank">flashxin</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/YesYourHighness" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2025/08/10/AI/RAG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="http://img.yesmylord.cn//1644852537960.jpg">
      <meta itemprop="name" content="Hynis">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hynis">
      <meta itemprop="description" content="A blog about IT knowledge">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="RAG | Hynis">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RAG
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-10 01:54:36 / 修改时间：22:25:12" itemprop="dateCreated datePublished" datetime="2025-08-10T01:54:36+08:00">2025-08-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RAG/" itemprop="url" rel="index"><span itemprop="name">RAG</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <center>
引言：RAG——向量数据库+大语言模型
</center>


<span id="more"></span>

<h1 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h1><p><strong>RAG（Retrieval-Augmented Generation）</strong>是一种结合了<strong>信息检索</strong>和<strong>文本生成</strong>的AI技术。</p>
<p>它通过在生成回答之前先检索相关的外部知识，来提高大语言模型回答的准确性和时效性。</p>
<p>简单来说RAG是一个<strong>向量数据库+嵌入模型</strong></p>
<h1 id="RAG的核心组件"><a href="#RAG的核心组件" class="headerlink" title="RAG的核心组件"></a>RAG的核心组件</h1><p>RAG系统主要由两个核心组件构成：</p>
<h2 id="1-检索器（Retriever）"><a href="#1-检索器（Retriever）" class="headerlink" title="1. 检索器（Retriever）"></a>1. 检索器（Retriever）</h2><ul>
<li><strong>功能</strong>：从大量文档或知识库中找到与用户问题最相关的信息</li>
<li><strong>常用技术</strong>：<strong>向量数据库</strong>（如Chroma、Pinecone、Weaviate）、稠密检索（Dense Retrieval）、稀疏检索（如BM25）</li>
</ul>
<h2 id="2-生成器（Generator）"><a href="#2-生成器（Generator）" class="headerlink" title="2. 生成器（Generator）"></a>2. 生成器（Generator）</h2><ul>
<li><strong>功能</strong>：基于检索到的相关信息生成最终回答</li>
<li><strong>常用模型</strong>：<strong>GPT系列</strong>（GPT-3.5、GPT-4）、Claude、LLaMA、其他大语言模型</li>
</ul>
<h1 id="RAG的工作流程"><a href="#RAG的工作流程" class="headerlink" title="RAG的工作流程"></a>RAG的工作流程</h1><h2 id="RAG系统架构全貌"><a href="#RAG系统架构全貌" class="headerlink" title="RAG系统架构全貌"></a>RAG系统架构全貌</h2><p>很多人以为RAG就是”向量数据库+模型”，这个理解基本正确，但实际上RAG是一个更复杂的系统。</p>
<p>RAG系统的典型工作流程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户问题 → 文档预处理 → 向量化 → 相似度检索 → 上下文增强 → 生成回答</span><br></pre></td></tr></table></figure>

<ol>
<li><strong>文档预处理</strong>：将文档切分成小块（chunking），清理和格式化文本，去除噪声内容</li>
<li><strong>向量化存储</strong>：使用嵌入模型（如OpenAI Embeddings、Sentence-BERT）将文档块转换为向量，然后存储到向量数据库中</li>
<li><strong>用户查询处理</strong>：将用户问题也转换为向量表示，在向量空间中计算相似度</li>
<li><strong>检索相关文档</strong>：找出最相关的Top-K个文档块，通常K=3到10之间</li>
<li><strong>上下文构建</strong>：将检索到的文档内容与用户问题组合，形成完整的prompt</li>
<li><strong>生成最终回答</strong>：大语言模型基于增强后的上下文生成回答、确保回答基于检索到的事实信息</li>
</ol>
<h3 id="完整架构图"><a href="#完整架构图" class="headerlink" title="完整架构图"></a>完整架构图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐</span><br><span class="line">│   文档输入       │    │   用户查询       │    │   最终回答       │</span><br><span class="line">│  (PDF/Word/     │    │  &quot;RAG是什么?&quot;   │    │  &quot;RAG是一种...&quot; │</span><br><span class="line">│   Web/...)      │    │                │    │                │</span><br><span class="line">└─────────┬───────┘    └─────────┬───────┘    └─────────┬───────┘</span><br><span class="line">          │                      │                      ▲</span><br><span class="line">          ▼                      ▼                      │</span><br><span class="line">┌─────────────────┐    ┌─────────────────┐              │</span><br><span class="line">│  文档处理模块    │    │  查询处理模块    │              │</span><br><span class="line">│ • 文本清理       │    │ • 查询理解       │              │</span><br><span class="line">│ • 文档分块       │    │ • 查询扩展       │              │</span><br><span class="line">│ • 元数据提取     │    │ • 意图识别       │              │</span><br><span class="line">└─────────┬───────┘    └─────────┬───────┘              │</span><br><span class="line">          │                      │                      │</span><br><span class="line">          ▼                      ▼                      │</span><br><span class="line">┌─────────────────┐    ┌─────────────────┐              │</span><br><span class="line">│  嵌入模型       │    │  嵌入模型       │              │</span><br><span class="line">│ • 文档向量化     │    │ • 查询向量化     │              │</span><br><span class="line">│ • 批量处理       │    │ • 实时处理       │              │</span><br><span class="line">└─────────┬───────┘    └─────────┬───────┘              │</span><br><span class="line">          │                      │                      │</span><br><span class="line">          ▼                      ▼                      │</span><br><span class="line">┌─────────────────┐    ┌─────────────────┐              │</span><br><span class="line">│  向量数据库     │◄───┤  检索引擎       │              │</span><br><span class="line">│ • 向量存储       │    │ • 相似度搜索     │              │</span><br><span class="line">│ • 索引管理       │    │ • 结果排序       │              │</span><br><span class="line">│ • 元数据存储     │    │ • 结果过滤       │              │</span><br><span class="line">└─────────────────┘    └─────────┬───────┘              │</span><br><span class="line">                                 │                      │</span><br><span class="line">                                 ▼                      │</span><br><span class="line">                       ┌─────────────────┐              │</span><br><span class="line">                       │  上下文构建     │              │</span><br><span class="line">                       │ • 文档整合       │              │</span><br><span class="line">                       │ • Prompt工程     │              │</span><br><span class="line">                       │ • 长度控制       │              │</span><br><span class="line">                       └─────────┬───────┘              │</span><br><span class="line">                                 │                      │</span><br><span class="line">                                 ▼                      │</span><br><span class="line">                       ┌─────────────────┐              │</span><br><span class="line">                       │  生成模型       │──────────────┘</span><br><span class="line">                       │ • 文本生成       │</span><br><span class="line">                       │ • 答案合成       │</span><br><span class="line">                       │ • 质量控制       │</span><br><span class="line">                       └─────────────────┘</span><br></pre></td></tr></table></figure>

<h3 id="为什么不只是”向量数据库-模型”？"><a href="#为什么不只是”向量数据库-模型”？" class="headerlink" title="为什么不只是”向量数据库+模型”？"></a>为什么不只是”向量数据库+模型”？</h3><p><strong>简化理解（适合学习）：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RAG ≈ 向量数据库 + 生成模型</span><br></pre></td></tr></table></figure>

<p><strong>完整实现（适合生产）：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RAG = 文档处理 + 嵌入模型 + 向量数据库 + 检索引擎 + </span><br><span class="line">      上下文管理 + 生成模型 + 质量控制 + 系统编排</span><br></pre></td></tr></table></figure>

<ol>
<li><p><strong>文档处理pipeline</strong></p>
<ul>
<li>不同格式文档的解析</li>
<li>文本清理和标准化</li>
<li>智能分块策略</li>
</ul>
</li>
<li><p><strong>检索优化</strong></p>
<ul>
<li>查询理解和扩展</li>
<li>多轮检索策略</li>
<li>结果重排序（Re-ranking）</li>
</ul>
</li>
<li><p><strong>上下文管理</strong></p>
<ul>
<li>长度控制（避免超出模型限制）</li>
<li>信息去重和整合</li>
<li>Prompt模板管理</li>
</ul>
</li>
<li><p><strong>质量控制</strong></p>
<ul>
<li>回答相关性检测</li>
<li>幻觉检测和过滤</li>
<li>置信度评估</li>
</ul>
</li>
<li><p><strong>系统编排</strong></p>
<ul>
<li>错误处理和恢复</li>
<li>性能监控</li>
<li>缓存机制</li>
</ul>
</li>
</ol>
<h3 id="各组件的作用占比"><a href="#各组件的作用占比" class="headerlink" title="各组件的作用占比"></a>各组件的作用占比</h3><table>
<thead>
<tr>
<th>组件</th>
<th>重要性</th>
<th>复杂度</th>
<th>对效果的影响</th>
</tr>
</thead>
<tbody><tr>
<td>向量数据库</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐</td>
<td>40%</td>
</tr>
<tr>
<td>生成模型</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐</td>
<td>30%</td>
</tr>
<tr>
<td>文档处理</td>
<td>⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
<td>15%</td>
</tr>
<tr>
<td>检索优化</td>
<td>⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
<td>10%</td>
</tr>
<tr>
<td>上下文管理</td>
<td>⭐⭐⭐</td>
<td>⭐⭐⭐</td>
<td>5%</td>
</tr>
</tbody></table>
<p><strong>关键洞察：</strong></p>
<ul>
<li>向量数据库和生成模型确实是核心，占70%的效果</li>
<li>但剩余30%的优化往往决定了系统的实用性</li>
<li>生产级RAG系统需要考虑所有组件的协同工作</li>
</ul>
<h1 id="开源框架"><a href="#开源框架" class="headerlink" title="开源框架"></a>开源框架</h1><ol>
<li><p><strong>LangChain</strong></p>
<ul>
<li>最流行的RAG开发框架</li>
<li>丰富的文档加载器和向量存储支持</li>
<li>易于集成各种LLM</li>
</ul>
</li>
<li><p><strong>LlamaIndex</strong></p>
<ul>
<li>专注于数据连接和索引</li>
<li>强大的查询引擎</li>
<li>支持复杂的文档结构</li>
</ul>
</li>
<li><p><strong>Haystack</strong></p>
<ul>
<li>Deepset开发的开源框架</li>
<li>企业级解决方案</li>
<li>强大的pipeline管理</li>
</ul>
</li>
</ol>
<h1 id="向量数据库"><a href="#向量数据库" class="headerlink" title="向量数据库"></a>向量数据库</h1><h2 id="什么是向量数据库？"><a href="#什么是向量数据库？" class="headerlink" title="什么是向量数据库？"></a>什么是向量数据库？</h2><p><strong>向量数据库（Vector Database）</strong> 是一种专门用于存储、索引和查询高维向量数据的数据库系统。</p>
<p>在RAG和AI应用中，它是实现语义搜索的核心技术。</p>
<h2 id="核心概念：向量、嵌入、相似度计算"><a href="#核心概念：向量、嵌入、相似度计算" class="headerlink" title="核心概念：向量、嵌入、相似度计算"></a>核心概念：向量、嵌入、相似度计算</h2><p><strong>1. 向量（Vector）</strong>：向量是一组数字组成的数组，代表数据的数学表示</p>
<p>示例：<code>[0.1, 0.8, -0.3, 0.5, ...]</code>（通常有几百到几千个维度）</p>
<p>可以将文本、图像等非结构化数据转换为计算机可以理解的数值形式（非结构化数据转为可理解的数值形式）</p>
<p><strong>2. 嵌入（Embedding）</strong>：使用AI模型将文本转换为向量的过程</p>
<p>语义相似的文本会产生相似的向量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;RAG是什么？&quot; → [0.1, 0.8, -0.3, 0.5, ...]</span><br><span class="line">&quot;什么是RAG？&quot; → [0.1, 0.7, -0.2, 0.6, ...]  (相似向量)</span><br><span class="line">&quot;今天天气好&quot; → [0.9, 0.1, 0.8, -0.4, ...] (不同向量)</span><br></pre></td></tr></table></figure>

<p><strong>3. 相似度计算</strong></p>
<p>相似度计算是向量数据库的核心，决定了能否准确找到相关文档。以下是三种主要的相似度计算方法：</p>
<p><strong>余弦相似度（Cosine Similarity）</strong> - 最常用的方法</p>
<ul>
<li><strong>原理</strong>：计算两个向量之间的夹角余弦值</li>
<li><strong>公式</strong>：<code>cos(θ) = (A·B) / (|A| × |B|)</code></li>
<li><strong>取值范围</strong>：-1 到 1，值越接近1越相似</li>
<li><strong>优势</strong>：不受向量长度影响，适合文本相似度</li>
</ul>
<p><strong>具体例子：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例向量</span></span><br><span class="line">vector_A = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]      <span class="comment"># &quot;RAG是什么&quot;</span></span><br><span class="line">vector_B = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]      <span class="comment"># &quot;什么是RAG&quot;  </span></span><br><span class="line">vector_C = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]      <span class="comment"># &quot;今天天气好&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算余弦相似度</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">v1, v2</span>):</span><br><span class="line">    dot_product = <span class="built_in">sum</span>(a*b <span class="keyword">for</span> a,b <span class="keyword">in</span> <span class="built_in">zip</span>(v1,v2))</span><br><span class="line">    norm_a = math.sqrt(<span class="built_in">sum</span>(a*a <span class="keyword">for</span> a <span class="keyword">in</span> v1))</span><br><span class="line">    norm_b = math.sqrt(<span class="built_in">sum</span>(a*a <span class="keyword">for</span> a <span class="keyword">in</span> v2))</span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm_a * norm_b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果对比</span></span><br><span class="line">sim_AB = cosine_similarity(vector_A, vector_B)  <span class="comment"># 0.99 (非常相似)</span></span><br><span class="line">sim_AC = cosine_similarity(vector_A, vector_C)  <span class="comment"># 0.53 (不太相似)</span></span><br></pre></td></tr></table></figure>

<p><strong>欧几里得距离（Euclidean Distance）</strong></p>
<ul>
<li><strong>原理</strong>：计算两点间的直线距离</li>
<li><strong>公式</strong>：<code>d = √[(x₁-x₂)² + (y₁-y₂)² + ... + (n₁-n₂)²]</code></li>
<li><strong>取值范围</strong>：0 到 ∞，值越小越相似</li>
<li><strong>特点</strong>：受向量长度和维度影响较大</li>
</ul>
<p><strong>具体例子：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">euclidean_distance</span>(<span class="params">v1, v2</span>):</span><br><span class="line">    <span class="keyword">return</span> math.sqrt(<span class="built_in">sum</span>((a-b)**<span class="number">2</span> <span class="keyword">for</span> a,b <span class="keyword">in</span> <span class="built_in">zip</span>(v1,v2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用相同的向量</span></span><br><span class="line">dist_AB = euclidean_distance(vector_A, vector_B)  <span class="comment"># 1.0 (距离较近)</span></span><br><span class="line">dist_AC = euclidean_distance(vector_A, vector_C)  <span class="comment"># 4.58 (距离较远)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为相似度 (0到1之间)</span></span><br><span class="line">sim_AB = <span class="number">1</span> / (<span class="number">1</span> + dist_AB)  <span class="comment"># 0.5</span></span><br><span class="line">sim_AC = <span class="number">1</span> / (<span class="number">1</span> + dist_AC)  <span class="comment"># 0.18</span></span><br></pre></td></tr></table></figure>

<p><strong>点积（Dot Product）</strong></p>
<ul>
<li><strong>原理</strong>：向量对应元素相乘后求和</li>
<li><strong>公式</strong>：<code>A·B = a₁×b₁ + a₂×b₂ + ... + aₙ×bₙ</code></li>
<li><strong>特点</strong>：计算最快，但受向量长度影响</li>
<li><strong>适用</strong>：向量已归一化的场景</li>
</ul>
<p><strong>具体例子：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dot_product</span>(<span class="params">v1, v2</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(a*b <span class="keyword">for</span> a,b <span class="keyword">in</span> <span class="built_in">zip</span>(v1,v2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用归一化向量</span></span><br><span class="line">norm_A = [<span class="number">0.27</span>, <span class="number">0.53</span>, <span class="number">0.80</span>]  <span class="comment"># 归一化后的向量A</span></span><br><span class="line">norm_B = [<span class="number">0.35</span>, <span class="number">0.70</span>, <span class="number">0.60</span>]  <span class="comment"># 归一化后的向量B</span></span><br><span class="line">norm_C = [<span class="number">0.95</span>, <span class="number">0.00</span>, <span class="number">0.24</span>]  <span class="comment"># 归一化后的向量C</span></span><br><span class="line"></span><br><span class="line">dot_AB = dot_product(norm_A, norm_B)  <span class="comment"># 0.85 (高相似度)</span></span><br><span class="line">dot_AC = dot_product(norm_A, norm_C)  <span class="comment"># 0.45 (低相似度)</span></span><br></pre></td></tr></table></figure>

<p><strong>实际应用中的选择：</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>优势</th>
<th>劣势</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>余弦相似度</td>
<td>不受长度影响，语义准确</td>
<td>计算稍复杂</td>
<td>文本搜索、推荐系统</td>
</tr>
<tr>
<td>欧几里得距离</td>
<td>直观易懂，几何意义明确</td>
<td>受维度影响大</td>
<td>图像识别、聚类分析</td>
</tr>
<tr>
<td>点积</td>
<td>计算最快，内存友好</td>
<td>需要归一化预处理</td>
<td>大规模实时搜索</td>
</tr>
</tbody></table>
<p><strong>RAG系统中的选择</strong>：</p>
<ul>
<li><strong>推荐</strong>：余弦相似度（90%的RAG系统都使用）</li>
<li><strong>原因</strong>：文本向量通常关注方向而非长度，余弦相似度能更好地捕捉语义相似性</li>
<li><strong>优化</strong>：可以预先归一化向量，然后使用点积代替余弦相似度计算</li>
</ul>
<hr>
<p><strong>PS：向量归一化（Vector Normalization）</strong></p>
<ul>
<li><strong>定义</strong>：将向量转换为单位向量（长度为1）的过程，保持方向不变</li>
<li><strong>目的</strong>：消除向量长度差异，让相似度计算更准确</li>
<li><strong>公式</strong>：<code>v_norm = v / |v|</code>，其中 <code>|v| = √(v₁² + v₂² + ... + vₙ²)</code></li>
</ul>
<p><strong>归一化示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始向量</span></span><br><span class="line">vector = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>]</span><br><span class="line">length = √(<span class="number">3</span>² + <span class="number">4</span>² + <span class="number">0</span>²) = √<span class="number">25</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化后</span></span><br><span class="line">normalized = [<span class="number">3</span>/<span class="number">5</span>, <span class="number">4</span>/<span class="number">5</span>, <span class="number">0</span>/<span class="number">5</span>] = [<span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">0.0</span>]</span><br><span class="line"><span class="comment"># 验证：√(0.6² + 0.8² + 0²) = 1 ✓</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本向量示例</span></span><br><span class="line">text_vector = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">length = √(<span class="number">1</span>² + <span class="number">2</span>² + <span class="number">3</span>² + <span class="number">4</span>²) = √<span class="number">30</span> = <span class="number">5.477</span></span><br><span class="line">normalized = [<span class="number">0.183</span>, <span class="number">0.365</span>, <span class="number">0.548</span>, <span class="number">0.730</span>]</span><br></pre></td></tr></table></figure>

<p><strong>归一化的优势：</strong></p>
<ul>
<li><strong>公平比较</strong>：不同长度的文本产生的向量可以公平比较</li>
<li><strong>计算简化</strong>：归一化后，余弦相似度 = 点积，计算更快</li>
<li><strong>数值稳定</strong>：避免因向量长度差异导致的数值不稳定</li>
<li><strong>语义专注</strong>：突出向量方向（语义），忽略大小（文本长度）</li>
</ul>
<p><strong>归一化对相似度计算的影响：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例：不同长度文本的向量对比</span></span><br><span class="line">short_text = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]      <span class="comment"># 短文本</span></span><br><span class="line">long_text = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]       <span class="comment"># 长文本（内容相似但更长）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 未归一化的点积</span></span><br><span class="line">dot_product = <span class="number">1</span>*<span class="number">2</span> + <span class="number">1</span>*<span class="number">2</span> + <span class="number">1</span>*<span class="number">2</span> = <span class="number">6</span>  <span class="comment"># 偏向长文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化后的向量</span></span><br><span class="line">short_norm = [<span class="number">0.577</span>, <span class="number">0.577</span>, <span class="number">0.577</span>]  <span class="comment"># 长度=1</span></span><br><span class="line">long_norm = [<span class="number">0.577</span>, <span class="number">0.577</span>, <span class="number">0.577</span>]   <span class="comment"># 长度=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化后的点积（等于余弦相似度）</span></span><br><span class="line">normalized_dot = <span class="number">0.577</span>*<span class="number">0.577</span> + <span class="number">0.577</span>*<span class="number">0.577</span> + <span class="number">0.577</span>*<span class="number">0.577</span> = <span class="number">1.0</span></span><br><span class="line"><span class="comment"># 结果：完全相似！这才是我们想要的结果</span></span><br></pre></td></tr></table></figure>

<h2 id="向量数据库工作原理"><a href="#向量数据库工作原理" class="headerlink" title="向量数据库工作原理"></a>向量数据库工作原理</h2><p><strong>存储阶段：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文档 → 分块 → 嵌入模型 → 向量 → 向量数据库</span><br></pre></td></tr></table></figure>

<p><strong>查询阶段：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户问题 → 嵌入模型 → 查询向量 → 相似度搜索 → 相关文档</span><br></pre></td></tr></table></figure>

<h3 id="为什么需要向量数据库？"><a href="#为什么需要向量数据库？" class="headerlink" title="为什么需要向量数据库？"></a>为什么需要向量数据库？</h3><p><strong>1. 语义搜索能力</strong></p>
<p>传统搜索：基于关键词匹配，而向量搜索基于语义理解</p>
<p>示例：查询”如何做饭？”能找到”烹饪方法”、”cooking tips”等相关内容</p>
<p><strong>2. 多语言支持</strong></p>
<ul>
<li>理解同义词和概念</li>
<li>支持跨语言查询</li>
<li>处理语言变体和方言</li>
</ul>
<p><strong>3. 高效检索</strong></p>
<ul>
<li>专门的索引结构（HNSW、IVF等）</li>
<li>快速的相似度计算</li>
<li>支持大规模数据检索</li>
</ul>
<h3 id="性能考虑"><a href="#性能考虑" class="headerlink" title="性能考虑"></a>性能考虑</h3><p><strong>1. 向量维度</strong></p>
<ul>
<li>更高维度：更精确，但存储和计算开销大</li>
<li>常见维度：384、768、1536（取决于嵌入模型）</li>
<li>需要在精度和性能间平衡</li>
</ul>
<p><strong>2. 索引类型</strong></p>
<ul>
<li><strong>HNSW</strong>：高精度，适合查询密集型应用</li>
<li><strong>IVF</strong>：适合大规模数据，支持近似搜索</li>
<li><strong>LSH</strong>：局部敏感哈希，适合快速近似搜索</li>
</ul>
<p><strong>3. 存储优化</strong></p>
<ul>
<li><strong>量化</strong>：减少存储空间，牺牲少量精度</li>
<li><strong>压缩</strong>：平衡精度和性能</li>
<li><strong>分片</strong>：支持分布式存储和查询</li>
</ul>
<h2 id="市场主流向量数据库对比"><a href="#市场主流向量数据库对比" class="headerlink" title="市场主流向量数据库对比"></a>市场主流向量数据库对比</h2><p>🏆 市场份额排名（2024年）</p>
<table>
<thead>
<tr>
<th>排名</th>
<th>产品</th>
<th>市场份额</th>
<th>类型</th>
<th>主要优势</th>
</tr>
</thead>
<tbody><tr>
<td>🥇</td>
<td>Pinecone</td>
<td>~35%</td>
<td>云托管</td>
<td>易用性、性能、企业功能</td>
</tr>
<tr>
<td>🥈</td>
<td>Chroma</td>
<td>~25%</td>
<td>开源</td>
<td>轻量级、Python友好</td>
</tr>
<tr>
<td>🥉</td>
<td>Weaviate</td>
<td>~15%</td>
<td>开源+云</td>
<td>功能丰富、多模态</td>
</tr>
<tr>
<td>4️⃣</td>
<td>Qdrant</td>
<td>~12%</td>
<td>开源+云</td>
<td>高性能、Rust编写</td>
</tr>
<tr>
<td>5️⃣</td>
<td>Milvus</td>
<td>~8%</td>
<td>开源+云</td>
<td>企业级、大规模</td>
</tr>
<tr>
<td>6️⃣</td>
<td>其他</td>
<td>~5%</td>
<td>各种</td>
<td>FAISS、OpenSearch等</td>
</tr>
</tbody></table>
<p><strong>🥇 最受欢迎：Pinecone</strong></p>
<ul>
<li>✅ 市场占有率最高，生态最成熟；托管服务，零运维成本；性能优秀，延迟低；企业级功能完善</li>
<li>❌ 成本较高，按使用量付费；供应商锁定风险</li>
</ul>
<p><strong>🥈 开源首选：Chroma</strong></p>
<ul>
<li>✅ 开源社区最活跃；极易上手，5分钟可部署；本地开发友好</li>
<li>❌ 功能相对简单；不适合大规模生产</li>
</ul>
<p>🤔 特殊提及：<strong>Neo4j不是传统向量数据库</strong>，而是图数据库，但：</p>
<ul>
<li>Neo4j 5.0+ 新增向量索引功能</li>
<li>主要用于图+向量混合查询</li>
<li>适合知识图谱+RAG结合的场景</li>
<li>如果只做向量搜索，不推荐使用</li>
</ul>
<h2 id="向量数据库演示代码"><a href="#向量数据库演示代码" class="headerlink" title="向量数据库演示代码"></a>向量数据库演示代码</h2><p>为了更好地理解向量数据库的工作原理，我们提供了一个简化的演示程序：</p>
<p>核心方法：嵌入 + 相似度计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 嵌入：向量转换</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_hash_embedding</span>(<span class="params">text, dimension=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;简单的哈希嵌入（仅用于演示概念）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [<span class="built_in">hash</span>(text + <span class="built_in">str</span>(i)) % <span class="number">100</span> / <span class="number">100.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dimension)]</span><br><span class="line"><span class="comment"># 相似度计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">v1, v2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算余弦相似度&quot;&quot;&quot;</span></span><br><span class="line">    dot = <span class="built_in">sum</span>(a*b <span class="keyword">for</span> a,b <span class="keyword">in</span> <span class="built_in">zip</span>(v1,v2))</span><br><span class="line">    norm1 = math.sqrt(<span class="built_in">sum</span>(a*a <span class="keyword">for</span> a <span class="keyword">in</span> v1))</span><br><span class="line">    norm2 = math.sqrt(<span class="built_in">sum</span>(a*a <span class="keyword">for</span> a <span class="keyword">in</span> v2))</span><br><span class="line">    <span class="keyword">if</span> norm1 == <span class="number">0</span> <span class="keyword">or</span> norm2 == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> dot / (norm1 * norm2)</span><br></pre></td></tr></table></figure>

<p>完整案例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vector_database_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;向量数据库工作原理演示&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;🔍 向量数据库概念演示&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">40</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 示例文档库</span></span><br><span class="line">    documents = [</span><br><span class="line">        <span class="string">&quot;RAG是什么？RAG是一种AI技术&quot;</span>,</span><br><span class="line">        <span class="string">&quot;向量数据库用于存储向量数据&quot;</span>, </span><br><span class="line">        <span class="string">&quot;Python是一种编程语言&quot;</span>,</span><br><span class="line">        <span class="string">&quot;机器学习需要大量数据&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 文档向量化</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;📚 步骤1：文档向量化&quot;</span>)</span><br><span class="line">    doc_vectors = []</span><br><span class="line">    <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(documents):</span><br><span class="line">        vector = simple_hash_embedding(doc)</span><br><span class="line">        doc_vectors.append(vector)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;文档<span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: <span class="subst">&#123;doc&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;  向量: <span class="subst">&#123;[<span class="built_in">round</span>(v, <span class="number">3</span>) <span class="keyword">for</span> v <span class="keyword">in</span> vector]&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 查询处理</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n🔍 步骤2：查询处理&quot;</span>)</span><br><span class="line">    queries = [<span class="string">&quot;什么是RAG？&quot;</span>, <span class="string">&quot;数据库&quot;</span>, <span class="string">&quot;编程&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> query <span class="keyword">in</span> queries:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;\n查询: &#x27;<span class="subst">&#123;query&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">        query_vector = simple_hash_embedding(query)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;查询向量: <span class="subst">&#123;[<span class="built_in">round</span>(v, <span class="number">3</span>) <span class="keyword">for</span> v <span class="keyword">in</span> query_vector]&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 相似度计算和排序</span></span><br><span class="line">        similarities = []</span><br><span class="line">        <span class="keyword">for</span> i, doc_vector <span class="keyword">in</span> <span class="built_in">enumerate</span>(doc_vectors):</span><br><span class="line">            sim = cosine_similarity(query_vector, doc_vector)</span><br><span class="line">            similarities.append((sim, i, documents[i]))</span><br><span class="line">        </span><br><span class="line">        similarities.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;📊 相似度排序结果:&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> sim, idx, doc <span class="keyword">in</span> similarities:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;  文档<span class="subst">&#123;idx+<span class="number">1</span>&#125;</span>: <span class="subst">&#123;sim:<span class="number">.3</span>f&#125;</span> - <span class="subst">&#123;doc&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;🎯 最相关文档: 文档<span class="subst">&#123;similarities[<span class="number">0</span>][<span class="number">1</span>]+<span class="number">1</span>&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    vector_database_demo()</span><br></pre></td></tr></table></figure>

<p><strong>运行示例输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">🔍 向量数据库概念演示</span><br><span class="line">========================================</span><br><span class="line">📚 步骤1：文档向量化</span><br><span class="line">文档1: RAG是什么？RAG是一种AI技术</span><br><span class="line">  向量: [0.68, 0.8, 0.4, 0.08, 0.72]</span><br><span class="line">文档2: 向量数据库用于存储向量数据</span><br><span class="line">  向量: [0.9, 0.71, 0.2, 0.59, 0.13]</span><br><span class="line"></span><br><span class="line">🔍 步骤2：查询处理</span><br><span class="line"></span><br><span class="line">查询: &#x27;什么是RAG？&#x27;</span><br><span class="line">查询向量: [0.34, 0.95, 0.9, 0.65, 0.74]</span><br><span class="line">📊 相似度排序结果:</span><br><span class="line">  文档1: 0.866 - RAG是什么？RAG是一种AI技术</span><br><span class="line">  文档2: 0.748 - 向量数据库用于存储向量数据</span><br><span class="line">🎯 最相关文档: 文档1</span><br></pre></td></tr></table></figure>

<p>这个演示展示了：</p>
<ol>
<li><strong>文档向量化</strong>：将文本转换为数值向量</li>
<li><strong>查询向量化</strong>：用户问题也转换为向量</li>
<li><strong>相似度计算</strong>：通过数学方法找到最相关的文档</li>
<li><strong>结果排序</strong>：按相似度高低排列搜索结果</li>
</ol>
<p>虽然这是一个简化的示例，但它准确地反映了向量数据库在RAG系统中的核心工作原理。</p>
<h1 id="语言大模型选择"><a href="#语言大模型选择" class="headerlink" title="语言大模型选择"></a>语言大模型选择</h1><p>RAG系统中的生成模型（LLM）负责基于检索到的上下文生成最终回答。不同模型有各自的特点和适用场景。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>成本</th>
<th>中文能力</th>
<th>上下文长度</th>
<th>RAG适配度</th>
<th>推荐指数</th>
</tr>
</thead>
<tbody><tr>
<td><strong>DeepSeek-V3</strong></td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐⭐</td>
<td>64K</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>⭐⭐</td>
<td>⭐⭐⭐⭐</td>
<td>128K</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td>Claude-3.5</td>
<td>⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
<td>200K</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>⭐⭐⭐⭐</td>
<td>⭐⭐⭐</td>
<td>16K</td>
<td>⭐⭐⭐⭐</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td>本地模型</td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐</td>
<td>变化</td>
<td>⭐⭐⭐</td>
<td>⭐⭐⭐</td>
</tr>
</tbody></table>
<h1 id="LangChain框架深度解析"><a href="#LangChain框架深度解析" class="headerlink" title="LangChain框架深度解析"></a>LangChain框架深度解析</h1><h2 id="什么是LangChain？"><a href="#什么是LangChain？" class="headerlink" title="什么是LangChain？"></a>什么是LangChain？</h2><p><strong>LangChain</strong> 是目前最流行的<strong>大语言模型应用开发框架</strong>，由Harrison Chase在2022年创建。它的核心理念是**”链式组合”**，将不同的组件（如模型、数据源、工具等）串联起来，构建复杂的AI应用。</p>
<h2 id="LangChain架构体系"><a href="#LangChain架构体系" class="headerlink" title="LangChain架构体系"></a>LangChain架构体系</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">用户查询 → 🔗 RetrievalQA Chain</span><br><span class="line">    ↓</span><br><span class="line">📄 DirectoryLoader → ✂️ RecursiveCharacterTextSplitter</span><br><span class="line">    ↓</span><br><span class="line">🧮 Embeddings Model → 🗄️ Vector Store (Chroma)</span><br><span class="line">    ↓</span><br><span class="line">🔍 Similarity Retriever → 📝 Custom Prompt Template</span><br><span class="line">    ↓</span><br><span class="line">🤖 LLM Model (DeepSeek) → 💬 Structured Response</span><br></pre></td></tr></table></figure>

<h2 id="LangChain关键组件详解"><a href="#LangChain关键组件详解" class="headerlink" title="LangChain关键组件详解"></a>LangChain关键组件详解</h2><h3 id="1-📄-Document-Loaders（文档加载器）"><a href="#1-📄-Document-Loaders（文档加载器）" class="headerlink" title="1. 📄 Document Loaders（文档加载器）"></a>1. 📄 Document Loaders（文档加载器）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> (</span><br><span class="line">    DirectoryLoader,    <span class="comment"># 目录批量加载</span></span><br><span class="line">    PDFLoader,         <span class="comment"># PDF文档</span></span><br><span class="line">    CSVLoader,         <span class="comment"># CSV数据</span></span><br><span class="line">    WebBaseLoader,     <span class="comment"># 网页内容</span></span><br><span class="line">    GitHubIssuesLoader <span class="comment"># GitHub问题</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 支持100+种数据源</span></span><br><span class="line">loader = DirectoryLoader(</span><br><span class="line">    <span class="string">&quot;documents/&quot;</span>,</span><br><span class="line">    glob=<span class="string">&quot;**/*.txt&quot;</span>,</span><br><span class="line">    loader_cls=TextLoader,</span><br><span class="line">    show_progress=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="2-✂️-Text-Splitters（文本分割器）"><a href="#2-✂️-Text-Splitters（文本分割器）" class="headerlink" title="2. ✂️ Text Splitters（文本分割器）"></a>2. ✂️ Text Splitters（文本分割器）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> (</span><br><span class="line">    RecursiveCharacterTextSplitter,  <span class="comment"># 递归分割（推荐）</span></span><br><span class="line">    TokenTextSplitter,               <span class="comment"># 按token分割</span></span><br><span class="line">    MarkdownHeaderTextSplitter,      <span class="comment"># Markdown标题分割</span></span><br><span class="line">    CodeTextSplitter                 <span class="comment"># 代码文件分割</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">1000</span>,</span><br><span class="line">    chunk_overlap=<span class="number">200</span>,</span><br><span class="line">    separators=[<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;\n&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot; &quot;</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="3-🗄️-Vector-Stores（向量存储）"><a href="#3-🗄️-Vector-Stores（向量存储）" class="headerlink" title="3. 🗄️ Vector Stores（向量存储）"></a>3. 🗄️ Vector Stores（向量存储）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> (</span><br><span class="line">    Chroma,      <span class="comment"># 开源，轻量级</span></span><br><span class="line">    Pinecone,    <span class="comment"># 云服务，可扩展</span></span><br><span class="line">    FAISS,       <span class="comment"># Facebook AI，高性能</span></span><br><span class="line">    Weaviate,    <span class="comment"># 功能丰富</span></span><br><span class="line">    Qdrant       <span class="comment"># 高性能，Rust实现</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">vectorstore = Chroma.from_documents(</span><br><span class="line">    documents=splits,</span><br><span class="line">    embedding=embeddings,</span><br><span class="line">    persist_directory=<span class="string">&quot;./chroma_db&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="4-🔗-Chains（链）"><a href="#4-🔗-Chains（链）" class="headerlink" title="4. 🔗 Chains（链）"></a>4. 🔗 Chains（链）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> (</span><br><span class="line">    RetrievalQA,              <span class="comment"># 检索问答</span></span><br><span class="line">    ConversationalRetrievalChain,  <span class="comment"># 对话检索</span></span><br><span class="line">    LLMChain,                 <span class="comment"># 基础LLM链</span></span><br><span class="line">    SequentialChain           <span class="comment"># 顺序链</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同的链类型</span></span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">&quot;stuff&quot;</span>,      <span class="comment"># 合并所有文档</span></span><br><span class="line">    <span class="comment"># chain_type=&quot;map_reduce&quot;, # 先处理后合并</span></span><br><span class="line">    <span class="comment"># chain_type=&quot;refine&quot;,     # 逐步精炼</span></span><br><span class="line">    retriever=retriever</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="5-🤖-Agents（智能体）"><a href="#5-🤖-Agents（智能体）" class="headerlink" title="5. 🤖 Agents（智能体）"></a>5. 🤖 Agents（智能体）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> (</span><br><span class="line">    AgentExecutor,</span><br><span class="line">    create_react_agent,</span><br><span class="line">    create_tool_calling_agent</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent可以使用工具和推理</span></span><br><span class="line">agent = create_react_agent(</span><br><span class="line">    llm=llm,</span><br><span class="line">    tools=[search_tool, calculator_tool],</span><br><span class="line">    prompt=agent_prompt</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/RAG/" rel="tag"><i class="fa fa-tag"></i> RAG</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/08/04/Es/Elasticsearch%E5%AE%8C%E5%85%A8%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/" rel="prev" title="Elasticsearch学习指南">
                  <i class="fa fa-chevron-left"></i> Elasticsearch学习指南
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">晋ICP备 - 20007839号-1 </a>
  </div>

<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hynis</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">1.3m</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">19:08</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

</body>
</html>
