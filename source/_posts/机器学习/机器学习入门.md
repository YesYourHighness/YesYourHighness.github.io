---
title: 机器学习入门
date: 2022-07-09 20:44:04
tags: 
- 机器学习
categories: 
- 机器学习

---

<center>
引言：机器学习入门。入坑机器学习
</center>


<!--more-->

# 机器学习入门

吴恩达老师的机器学习入门课，本文的目的是入门，了解基本的概念，梳理整个结构，对具体的算法内容不求甚解。

## 什么是机器学习

> 机器学习 Machine Learning：
>
> 在没有具体的编程的情况下（without being explicitly programmed）使计算机有学习能力的研究领域

### 机器学习分类

机器学习有以下几种：

- **Supervised Learning 监督学习**：所谓监督，就是会给出一个集合，并且会给出这个集合内的部分元素（right answers）
- **Unsupervised Learning 非监督学习**：数据没有任何标签，我们需要找数据可能存在的结构
- **Reinforcement Learning 强化学习**： 有**奖惩**（rewards and penalties）机制的交互式学习

### 监督学习

监督学习算法有两种形式：

- **回归问题 Regression**：设法预测连续值的属性（比如预测房子的价格）
- **分类问题 Classification**：设法对其划分种类（比如预测癌症的恶性与良性、预测是否下雨）
  - Multiclass classification 多类分类：比如分为热狗、披萨、面条
  - Binary classification 二元分类，总共只有两种分类
    - 热狗 & 不是热狗
    - 积极的 & 消极的（比如评论）


### 非监督学习

非监督学习算法的类别大致有以下几种

**聚类**（Clustering Algorithm）：

- 社交网络的应用：判断一个人的社交圈子，判断他们哪些人相互认识
- 管理大型的计算机集群：可能存在将部分计算机放在一起，可以减少内部传输消耗，增加效率
- 给用户画像，将用户细分到不同的市场中去

**鸡尾酒会算法**（Cocktail party algorithm）：从源数据中分离出不同的数据

- 分离音频

## 特征 Features

特征有两种：

- 定性特征
- 定量特征
  - 连续的
  - 离散的



## 监督学习Demo

吴老师以一个房子的大小与价格的demo为例，串了一下监督学习的整个过程

1、数据：有一个 房子大小、价格的数据表（这也是监督学习的基础）

2、**假设函数**：假设模拟为 `h=a + bx`（模拟为了简单的线性函数）

3、确定一个**代价函数**（通常选用**平方误差函数** Square Error Function，对于线性回归问题，平方误差函数是一个很常用的手段）

4、**优化目标**：尽量使代价函数**小**，选择不同的参数，尽可能的使代价函数小，代价函数越小说明我们拟合的越好（吴老师用到了等高线图）

如何优化目标，我们可以使用**梯度下降算法**

### 梯度下降算法

为了方便的找到使代价函数最小的参数值，我们可以用**梯度下降算法（Gradient descent）**

> 梯度下降算法的大致策略是：
>
> 1、 给定参数的初始值（初始值是多少并不重要）
>
> 2、不断的改变参数值，减小代价函数，直到我们找到最小

![梯度下降算法解释](http://img.yesmylord.cn//image-20220708214333584.png)

相当于这两座山峰，我们需要从山坡上（初始值），找到最快下山的一条路（最小值）；然后我们再改变一开始的位置，然后再次寻找最快的路；

不同的路可能会找到不同的最低点（可能会找到不同的局部最优解）

> 梯度算法的特点是：不同的初始值可能会找到不同的局部最优解

数学原理就是：

![梯度下降算法](http://img.yesmylord.cn//image-20220708223332567.png)

（α表示学习率、J表示代价函数，j表示不同的参数代号、`:=`表示赋值，此式的意思是不断的修正θj的值，括号的内容表示要**同步更新参数**）

![同步更新参数与非同步更新参数](http://img.yesmylord.cn//image-20220708223717637.png)

（不同步更新的话，可能会用新的θ0值去算新的θ1值，如图右侧所示）

如图梯度下降算法示，**α代表学习率 Learning Rate**，它的大小表示我们是以大步子下山还是小步子下山。

α越小，下山就会慢（趋近最小值的速度就会变慢）

α越大，可能会错过最低点（但是速度很快）

> 有意思的是，我们并不需要频繁修改学习率α，因为当趋近于局部最低点的时候，我们的斜率，会变的很小很小（趋近于0，因为一条横线的斜率就是0），因此我们的步子就会自动的变小

这样的梯度下降算法，也叫**Batch 梯度下降算法**，它意味着：每次我们下降梯度，都会遍历所有的训练集

当然也有其他的梯度下降算法没有遍历所有的训练集，因此他们也不叫这个名字。





